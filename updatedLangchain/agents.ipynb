{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36a532f",
   "metadata": {},
   "source": [
    "#### Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46637044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b9ceb",
   "metadata": {},
   "source": [
    "#### Creating a Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4faa17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Custom Tools\n",
    "\n",
    "def add(a:int,b:int)->int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a+b\n",
    "\n",
    "def multiply(a:int,b:int)->int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a*b\n",
    "\n",
    "def divide(a:int,b:int)->int:\n",
    "    \"\"\"Divide two numbers\"\"\"\n",
    "    return a/b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8e1b7",
   "metadata": {},
   "source": [
    "#### Langchain tools    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94c3dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper,GoogleSearchAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun,GoogleSearchRun\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "\n",
    "wikipedia_wrapper = WikipediaAPIWrapper(top_k_results=5,doc_content_chars_max=500)\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=wikipedia_wrapper)\n",
    "\n",
    "tavily = TavilySearchResults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42672906",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add,divide,multiply,arxiv,wikipedia,tavily]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89e4f2",
   "metadata": {},
   "source": [
    "#### Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0154aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6fa2ec",
   "metadata": {},
   "source": [
    "#### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe26d5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000205F645E470>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    tools=tools,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84c0649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43a0922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the attention all you need paper?', additional_kwargs={}, response_metadata={}, id='5c141490-7f68-49b7-b8af-da38790da3f3'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'rr0vh96dz', 'function': {'arguments': '{\"query\":\"attention all you need paper\"}', 'name': 'arxiv'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 711, 'total_tokens': 730, 'completion_time': 0.034137853, 'completion_tokens_details': None, 'prompt_time': 0.041185097, 'prompt_tokens_details': None, 'queue_time': 0.056741743, 'total_time': 0.07532295}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7270-5814-7c81-9ac3-d7a276a88b46-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'attention all you need paper'}, 'id': 'rr0vh96dz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 711, 'output_tokens': 19, 'total_tokens': 730}),\n",
       "  ToolMessage(content='Published: 2006-01-22\\nTitle: This paper has been withdrawn\\nAuthors: This paper has been withdrawn\\nSummary: This paper has been withdrawn\\n\\nPublished: 2025-12-03\\nTitle: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\\nAuthors: Anton Alyakin\\nSummary: The 2017 paper \\'\\'Attention Is All You Need\\'\\' introduced the Transformer architecture-and inadvertently spawned one of machine learning\\'s most persistent naming conventions. We analyze 717 arXiv preprints contain', name='arxiv', id='3cbe398d-7b6e-49fd-8ce7-3370541704f0', tool_call_id='rr0vh96dz'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '03ra8me14', 'function': {'arguments': '{\"query\":\"Attention Is All You Need paper\"}', 'name': 'wikipedia'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 857, 'total_tokens': 876, 'completion_time': 0.020693352, 'completion_tokens_details': None, 'prompt_time': 0.04950529, 'prompt_tokens_details': None, 'queue_time': 0.05534508, 'total_time': 0.070198642}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7270-6417-77e2-8456-f86fb9bd130b-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Attention Is All You Need paper'}, 'id': '03ra8me14', 'type': 'tool_call'}], usage_metadata={'input_tokens': 857, 'output_tokens': 19, 'total_tokens': 876}),\n",
       "  ToolMessage(content='Page: Attention Is All You Need\\nSummary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wi', name='wikipedia', id='e9ce3f75-83f4-47f1-b35b-ba985bf30046', tool_call_id='03ra8me14'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '8gjqpb514', 'function': {'arguments': '{\"query\":\"Bahdanau et al 2014\"}', 'name': 'wikipedia'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 984, 'total_tokens': 1005, 'completion_time': 0.077370444, 'completion_tokens_details': None, 'prompt_time': 0.068388784, 'prompt_tokens_details': None, 'queue_time': 0.055497905, 'total_time': 0.145759228}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7270-87aa-7561-b3c5-ba599f307747-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Bahdanau et al 2014'}, 'id': '8gjqpb514', 'type': 'tool_call'}], usage_metadata={'input_tokens': 984, 'output_tokens': 21, 'total_tokens': 1005}),\n",
       "  ToolMessage(content='Page: History of artificial neural networks\\nSummary: Artificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by biological neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics, the first implementation of ANNs was by psychologist Frank Rosenblatt, who developed the perceptron. Little research was conducted on ANNs in the 1970s and 1980s, with the AAAI calli', name='wikipedia', id='760d9807-1eca-45ad-b950-9e4934f0802a', tool_call_id='8gjqpb514'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '4vrzc71bp', 'function': {'arguments': '{\"query\":\"transformer paper\"}', 'name': 'arxiv'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1115, 'total_tokens': 1132, 'completion_time': 0.026722697, 'completion_tokens_details': None, 'prompt_time': 0.06456833, 'prompt_tokens_details': None, 'queue_time': 0.049826899, 'total_time': 0.091291027}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7270-aa1e-7972-9fc0-fba3a4619994-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'transformer paper'}, 'id': '4vrzc71bp', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1115, 'output_tokens': 17, 'total_tokens': 1132}),\n",
       "  ToolMessage(content='Published: 2006-01-22\\nTitle: This paper has been withdrawn\\nAuthors: This paper has been withdrawn\\nSummary: This paper has been withdrawn\\n\\nPublished: 2022-01-04\\nTitle: PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture\\nAuthors: Kai Han, Jianyuan Guo, Yehui Tang, Yunhe Wang\\nSummary: Transformer networks have achieved great progress for computer vision tasks. Transformer-in-Transformer (TNT) architecture utilizes inner transformer and outer transformer to extract bo', name='arxiv', id='bfaa1e8d-ee98-4fcc-ab92-bc9092cf2c0f', tool_call_id='4vrzc71bp'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '7be0g4b0f', 'function': {'arguments': '{\"query\":\"transformer paper\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 1254, 'total_tokens': 1274, 'completion_time': 0.025788824, 'completion_tokens_details': None, 'prompt_time': 0.084887559, 'prompt_tokens_details': None, 'queue_time': 0.056010081, 'total_time': 0.110676383}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7270-b21f-7f43-bee5-7b03437e426f-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'transformer paper'}, 'id': '7be0g4b0f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1254, 'output_tokens': 20, 'total_tokens': 1274}),\n",
       "  ToolMessage(content='[{\"title\": \"LLM Transformer Model Visually Explained\", \"url\": \"https://poloclub.github.io/transformer-explainer/\", \"content\": \"Transformer is a neural network architecture that has fundamentally changed the approach to Artificial Intelligence. Transformer was first introduced in the seminal paper \\\\\"Attention is All You Need\\\\\" in 2017 and has since become the go-to architecture for deep learning models, powering text-generative models like OpenAI\\'s GPT, Meta\\'s Llama, and Google\\'s Gemini. Beyond text, Transformer is also applied in audio generation, image recognition, protein structure prediction, and even game playing,\", \"score\": 0.7420672}, {\"title\": \"Transformer: A Novel Neural Network Architecture for Language ...\", \"url\": \"https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/\", \"content\": \"In our paper, we show that the Transformer outperforms both recurrent and convolutional models on academic English to German and English to French translation benchmarks. On top of higher translation quality, the Transformer requires less computation to train and is a much better fit for modern machine learning hardware, speeding up training by up to an order of magnitude.\\\\n\\\\n|  | [...] Neural networks, in particular recurrent neural networks (RNNs), are now at the core of the leading approaches to language understanding tasks such as language modeling, machine translation and question answering. In “Attention Is All You Need”, we introduce the Transformer, a novel neural network architecture based on a self-attention mechanism that we believe to be particularly well suited for language understanding. [...] give it a try, and look forward to seeing what the community can do with the Transformer.\", \"score\": 0.7173491}, {\"title\": \"A comprehensive survey on applications of transformers for deep ...\", \"url\": \"https://www.sciencedirect.com/science/article/abs/pii/S0957417423031688\", \"content\": \"In the research community, the importance of survey papers in providing a productive analysis, comparison, and contribution of progressive topics is widely recognized. Numerous survey papers on the topic of Transformers can be found in the literature. Most of them are addressing specific fields of application (Khan et al., 2022, Shamshad et al., 2023, Wang, Smetannikov et al., 2020), compare the performance of different model (Fournier et al., 2023, Selva et al., 2023, Tay et al., 2023), or [...] The paper presents a comprehensive survey on transformers for deep learning tasks.\\\\n •\\\\n\\\\n  The paper conducts a thorough analysis on highly effective models in five domains.\\\\n •\\\\n\\\\n  The paper classifies the models based on respective tasks using a proposed taxonomy.\\\\n •\\\\n\\\\n  The characteristics of the surveyed models are deeply explored and analyzed.\\\\n •\\\\n\\\\n  Future directions and challenges for transformer-based models are deciphered.\\\\n\\\\n## Abstract [...] of a comprehensive survey paper that encompasses its major applications across diverse domains. Therefore, this paper addresses this gap by conducting an extensive survey of proposed Transformer models spanning from 2017 to 2022. Our survey encompasses the identification of the top five application domains for Transformer-based models, namely: NLP, CV, multi-modality, audio and speech processing, and signal processing. We analyze the influence of highly impactful Transformer-based models within\", \"score\": 0.7154444}, {\"title\": \"How Transformers Work: A Detailed Exploration of ... - DataCamp\", \"url\": \"https://www.datacamp.com/tutorial/how-transformers-work\", \"content\": \"Originating from a 2017 research paper by Google, transformer models are one of the most recent and influential developments in the Machine Learning field. The first Transformer model was explained in the influential paper \\\\\"Attention is All You Need. [...] In a 2021 paper, Stanford scholars aptly termed these innovations foundation models, underscoring their foundational role in reshaping AI. Their work highlights how transformer models have not only revolutionized the field but also pushed the frontiers of what\\'s achievable in artificial intelligence, heralding a new era of possibilities. [...] And the final architecture is something similar like this (form the original paper)\\\\n\\\\nImage by the author. Original structure of Transformers.\\\\n\\\\nTo better understand this architecture, I recommend trying to apply a Transformer from scratch following this tutorial to build a transformer with PyTorch.\\\\n\\\\n## Real-Life Transformer Models\\\\n\\\\n### BERT\", \"score\": 0.6892434}, {\"title\": \"A Historical Survey of Advances in Transformer Architectures - MDPI\", \"url\": \"https://www.mdpi.com/2076-3417/14/10/4316\", \"content\": \"viewpoint. Therefore, the rest of the paper examines a historical perspective on the progression of notable transformer architectures in addition to discussing the state-of-the-art techniques and architectures for data of different types. [...] In recent times, transformer-based deep learning models have risen in prominence in the field of machine learning for a variety of tasks such as computer vision and text generation. Given this increased interest, a historical outlook at the development and rapid progression of transformer-based models becomes imperative in order to gain an understanding of the rise of this key architecture. This paper presents a survey of key works related to the early development and implementation of [...] Impactful works to be added to the survey were identified by searching online databases and scanning through the list of references within the main papers. The search was applied mainly to google scholar, OpenAI, Papers with Code, and arxiv as it was found that majority of the works on transformers were published through Arxiv. As the survey is based on the history of transformers, the search was not limited by year, but it was found that works were present only from the year 2017 to the\", \"score\": 0.6836946}]', name='tavily_search_results_json', id='c1445713-8fde-425c-8708-1c22a6caaf90', tool_call_id='7be0g4b0f', artifact={'query': 'transformer paper', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://poloclub.github.io/transformer-explainer/', 'title': 'LLM Transformer Model Visually Explained', 'content': 'Transformer is a neural network architecture that has fundamentally changed the approach to Artificial Intelligence. Transformer was first introduced in the seminal paper \"Attention is All You Need\" in 2017 and has since become the go-to architecture for deep learning models, powering text-generative models like OpenAI\\'s GPT, Meta\\'s Llama, and Google\\'s Gemini. Beyond text, Transformer is also applied in audio generation, image recognition, protein structure prediction, and even game playing,', 'score': 0.7420672, 'raw_content': None}, {'url': 'https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/', 'title': 'Transformer: A Novel Neural Network Architecture for Language ...', 'content': 'In our paper, we show that the Transformer outperforms both recurrent and convolutional models on academic English to German and English to French translation benchmarks. On top of higher translation quality, the Transformer requires less computation to train and is a much better fit for modern machine learning hardware, speeding up training by up to an order of magnitude.\\n\\n|  | [...] Neural networks, in particular recurrent neural networks (RNNs), are now at the core of the leading approaches to language understanding tasks such as language modeling, machine translation and question answering. In “Attention Is All You Need”, we introduce the Transformer, a novel neural network architecture based on a self-attention mechanism that we believe to be particularly well suited for language understanding. [...] give it a try, and look forward to seeing what the community can do with the Transformer.', 'score': 0.7173491, 'raw_content': None}, {'url': 'https://www.sciencedirect.com/science/article/abs/pii/S0957417423031688', 'title': 'A comprehensive survey on applications of transformers for deep ...', 'content': 'In the research community, the importance of survey papers in providing a productive analysis, comparison, and contribution of progressive topics is widely recognized. Numerous survey papers on the topic of Transformers can be found in the literature. Most of them are addressing specific fields of application (Khan et al., 2022, Shamshad et al., 2023, Wang, Smetannikov et al., 2020), compare the performance of different model (Fournier et al., 2023, Selva et al., 2023, Tay et al., 2023), or [...] The paper presents a comprehensive survey on transformers for deep learning tasks.\\n •\\n\\n  The paper conducts a thorough analysis on highly effective models in five domains.\\n •\\n\\n  The paper classifies the models based on respective tasks using a proposed taxonomy.\\n •\\n\\n  The characteristics of the surveyed models are deeply explored and analyzed.\\n •\\n\\n  Future directions and challenges for transformer-based models are deciphered.\\n\\n## Abstract [...] of a comprehensive survey paper that encompasses its major applications across diverse domains. Therefore, this paper addresses this gap by conducting an extensive survey of proposed Transformer models spanning from 2017 to 2022. Our survey encompasses the identification of the top five application domains for Transformer-based models, namely: NLP, CV, multi-modality, audio and speech processing, and signal processing. We analyze the influence of highly impactful Transformer-based models within', 'score': 0.7154444, 'raw_content': None}, {'url': 'https://www.datacamp.com/tutorial/how-transformers-work', 'title': 'How Transformers Work: A Detailed Exploration of ... - DataCamp', 'content': 'Originating from a 2017 research paper by Google, transformer models are one of the most recent and influential developments in the Machine Learning field. The first Transformer model was explained in the influential paper \"Attention is All You Need. [...] In a 2021 paper, Stanford scholars aptly termed these innovations foundation models, underscoring their foundational role in reshaping AI. Their work highlights how transformer models have not only revolutionized the field but also pushed the frontiers of what\\'s achievable in artificial intelligence, heralding a new era of possibilities. [...] And the final architecture is something similar like this (form the original paper)\\n\\nImage by the author. Original structure of Transformers.\\n\\nTo better understand this architecture, I recommend trying to apply a Transformer from scratch following this tutorial to build a transformer with PyTorch.\\n\\n## Real-Life Transformer Models\\n\\n### BERT', 'score': 0.6892434, 'raw_content': None}, {'url': 'https://www.mdpi.com/2076-3417/14/10/4316', 'title': 'A Historical Survey of Advances in Transformer Architectures - MDPI', 'content': 'viewpoint. Therefore, the rest of the paper examines a historical perspective on the progression of notable transformer architectures in addition to discussing the state-of-the-art techniques and architectures for data of different types. [...] In recent times, transformer-based deep learning models have risen in prominence in the field of machine learning for a variety of tasks such as computer vision and text generation. Given this increased interest, a historical outlook at the development and rapid progression of transformer-based models becomes imperative in order to gain an understanding of the rise of this key architecture. This paper presents a survey of key works related to the early development and implementation of [...] Impactful works to be added to the survey were identified by searching online databases and scanning through the list of references within the main papers. The search was applied mainly to google scholar, OpenAI, Papers with Code, and arxiv as it was found that majority of the works on transformers were published through Arxiv. As the survey is based on the history of transformers, the search was not limited by year, but it was found that works were present only from the year 2017 to the', 'score': 0.6836946, 'raw_content': None}], 'response_time': 1.2, 'request_id': 'a2db67cd-c267-4dec-9b3e-004aefd508da'}),\n",
       "  AIMessage(content='The transformer paper is a seminal paper in the field of artificial intelligence, introducing a new neural network architecture based on a self-attention mechanism that has become the main architecture of many AI models. The paper was first introduced in 2017 and has since become widely used in various applications, including natural language processing, computer vision, and audio processing. The transformer architecture has been shown to be particularly effective in tasks that require understanding and generating human language, and has been used in many state-of-the-art models, including BERT and RoBERTa. The paper has also been widely cited and has had a significant impact on the field of AI, leading to a new era of possibilities in artificial intelligence.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 2509, 'total_tokens': 2649, 'completion_time': 0.245039875, 'completion_tokens_details': None, 'prompt_time': 0.142343326, 'prompt_tokens_details': None, 'queue_time': 0.051139264, 'total_time': 0.387383201}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7270-bb35-7a23-88a8-9b098fef3313-0', usage_metadata={'input_tokens': 2509, 'output_tokens': 140, 'total_tokens': 2649})]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "agent.invoke({\n",
    "    \"messages\":[{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is the attention all you need paper?\"\n",
    "    }]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a85c9813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the result of 2+2?', additional_kwargs={}, response_metadata={}, id='8b0654a8-202f-42fc-9016-5dd8d52d0a59'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '5qzt3yqmq', 'function': {'arguments': '{\"a\":2,\"b\":2}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 712, 'total_tokens': 730, 'completion_time': 0.027702186, 'completion_tokens_details': None, 'prompt_time': 0.039728916, 'prompt_tokens_details': None, 'queue_time': 0.055379534, 'total_time': 0.067431102}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7271-4649-7430-88de-e55a4f18c1f4-0', tool_calls=[{'name': 'add', 'args': {'a': 2, 'b': 2}, 'id': '5qzt3yqmq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 712, 'output_tokens': 18, 'total_tokens': 730}),\n",
       "  ToolMessage(content='4', name='add', id='7ee8009c-da78-42cb-ada1-8bb45d4d23aa', tool_call_id='5qzt3yqmq'),\n",
       "  AIMessage(content='The result of 2+2 is 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 741, 'total_tokens': 753, 'completion_time': 0.015089979, 'completion_tokens_details': None, 'prompt_time': 0.041576759, 'prompt_tokens_details': None, 'queue_time': 0.049879371, 'total_time': 0.056666738}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7271-47d0-70d1-b96d-f06e6172c5a8-0', usage_metadata={'input_tokens': 741, 'output_tokens': 12, 'total_tokens': 753})]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "agent.invoke({\n",
    "    \"messages\":[{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is the result of 2+2?\"\n",
    "    }]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37da1669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is machine learning using tavily?', additional_kwargs={}, response_metadata={}, id='75c3a146-feec-4a74-a5f4-7cb29ab2ad7f'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'vwdvw506g', 'function': {'arguments': '{\"query\":\"machine learning definition tavily\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 710, 'total_tokens': 732, 'completion_time': 0.02555581, 'completion_tokens_details': None, 'prompt_time': 0.063634679, 'prompt_tokens_details': None, 'queue_time': 0.056733641, 'total_time': 0.089190489}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7271-f739-7d31-8612-81ad1657648a-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'machine learning definition tavily'}, 'id': 'vwdvw506g', 'type': 'tool_call'}], usage_metadata={'input_tokens': 710, 'output_tokens': 22, 'total_tokens': 732}),\n",
       "  ToolMessage(content='[{\"title\": \"Build dynamic web research agents with the Strands Agents SDK ...\", \"url\": \"https://aws.amazon.com/blogs/machine-learning/build-dynamic-web-research-agents-with-the-strands-agents-sdk-and-tavily/\", \"content\": \"Tavily is an API-first web intelligence layer designed specifically for LLM agents, powering real-time search, high-fidelity content extraction, and structured web crawling. Built for developers building AI-based systems, Tavily is engineered for precision, speed, and modularity. It offers a seamless integration experience for agent frameworks like Strands Agents.Tavily’s API is an enterprise-grade infrastructure layer trusted by leading AI companies. It combines robust capabilities with\", \"score\": 0.99948066}, {\"title\": \"Tavily - The Web Access Layer for AI Agents\", \"url\": \"https://tavily.com/\", \"content\": \"Tavily is an AI search engine built for this purpose. Its fast, secure Search, Extract, and Crawl APIs deliver fresh, high‑quality results with citations, enabling any AI application—chatbots, agents, RAG systems, monitoring, and enrichment pipelines—to consistently benefit from current, reliable web knowledge. [...] Scalable: Tavily is built to scale as your usage grows, making it a reliable solution for both startups and enterprise customers. [...] Empower your chatbots to deliver precise, up-to-date responses by accessing a wide range of information through Tavily’s robust search capabilities.\\\\n\\\\nGet started\\\\n\\\\n## Blazing fast and production-ready.\\\\n\\\\nTavily is the web layer that supplies enterprise-grade agents with  \\\\n fast, real-time web data through simple, agent-first APIs.\\\\n\\\\nLearn MoreContact Us\\\\n\\\\nBenchmarks\\\\n\\\\n### Web Search Driven by Research\\\\n\\\\n##### About this benchmark\", \"score\": 0.9985562}, {\"title\": \"Tavily is the ultimate AI web search tool for research! This powerful ...\", \"url\": \"https://www.linkedin.com/posts/ohee-syed_tavily-is-the-ultimate-ai-web-search-tool-activity-7270068088513998848-9iGu\", \"content\": \"— you need vectors. #AI#VectorDatabase#Embeddings#SemanticSearch#MachineLearning#RAG#LLM#TechInnovation [...] Tavily is the ultimate AI web search tool for research! This powerful search API is specifically engineered for AI agents and LLMs. Here\\'s what makes it amazing: It outperforms traditional search APIs by delivering AI-optimized results that dramatically reduce hallucinations and bias. The magic happens in a single API call: - Real-time, factual results - Multi-source verification - Intelligent query processing and nuanced answers - Data relevance based on scores for LLMs What sets it apart from [...] Tavily is the ultimate AI web search tool for research! This powerful search API is specifically engineered for AI agents and LLMs. Here\\'s what makes it amazing: It outperforms traditional search… | Ohee Syed\\\\n\\\\nAgree & Join LinkedIn\\\\n\\\\nBy clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.\\\\n\\\\nSkip to main contentLinkedIn\\\\n   Top Content\\\\n   People\\\\n   Learning\\\\n   Jobs\\\\n   Games\\\\n\\\\nSign inJoin for free\\\\n\\\\nPlay Video\\\\n\\\\nVideo Player is loading.\", \"score\": 0.99608517}, {\"title\": \"Tavily — Integrating the Internet into your AI Application - Medium\", \"url\": \"https://medium.com/@prach8520/tavily-integrating-the-internet-into-your-ai-application-de14c6a7f8d9\", \"content\": \"Enter Tavily, an AI-powered search and retrieval tool that outsources the entire RAG component to a managed provider, making your LLM pipeline more streamlined and automatically up to date. In this guide, we’ll explore why Tavily is a game-changer for MLOps and walk through how to build a chatbot that leverages Tavily for retrieval to augment a movie recommendation app.\\\\n\\\\n## Why does AI Need the Internet? [...] ``` [...] ```\", \"score\": 0.99390244}, {\"title\": \"Tavily - AI for Data Processing, Research Automation ... - Agent Locker\", \"url\": \"https://www.agentlocker.ai/agent/tavily\", \"content\": \"AI agents that use historical data and machine learning algorithms to predict future outcomes. These agents analyze patterns, trends, and correlations to provide insights for decision-making in areas such as marketing, finance, operations, and risk management.\\\\n\\\\n## Agent Use Cases\\\\n\\\\n### Data Processing\\\\n\\\\nAutomated handling, transformation, and analysis of large datasets using AI algorithms ### Research Automation [...] AI agents designed to support research activities by gathering relevant information, analyzing data, summarizing findings, and even generating hypotheses. These agents assist in academic research, market analysis, scientific studies, and more, helping researchers to save time and focus on critical aspects of their work.\\\\n\\\\nTask Automation [...] Tavily is an AI-driven specialized search engine designed to help users gather relevant data from a variety of sources, streamlining the research process. By using the platform, individuals can quickly extract useful insights from webpages, documents, and external databases. It simplifies knowledge gathering, making it easy to automate research and search tasks.\\\\n\\\\n## Agentic\\\\n\\\\n3 upvotes\\\\n\\\\nAdd to Locker\\\\n\\\\nVisit Website\\\\n\\\\nView Documentation\\\\n\\\\nDeveloper\\\\n\\\\nTavily Inc\\\\n\\\\nAdded\\\\n\\\\n385 days ago\\\\n\\\\n### Analytics\", \"score\": 0.9876639}]', name='tavily_search_results_json', id='a65ad282-5ba5-490a-87ff-45d30c95b2a1', tool_call_id='vwdvw506g', artifact={'query': 'machine learning definition tavily', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://aws.amazon.com/blogs/machine-learning/build-dynamic-web-research-agents-with-the-strands-agents-sdk-and-tavily/', 'title': 'Build dynamic web research agents with the Strands Agents SDK ...', 'content': 'Tavily is an API-first web intelligence layer designed specifically for LLM agents, powering real-time search, high-fidelity content extraction, and structured web crawling. Built for developers building AI-based systems, Tavily is engineered for precision, speed, and modularity. It offers a seamless integration experience for agent frameworks like Strands Agents.Tavily’s API is an enterprise-grade infrastructure layer trusted by leading AI companies. It combines robust capabilities with', 'score': 0.99948066, 'raw_content': None}, {'url': 'https://tavily.com/', 'title': 'Tavily - The Web Access Layer for AI Agents', 'content': 'Tavily is an AI search engine built for this purpose. Its fast, secure Search, Extract, and Crawl APIs deliver fresh, high‑quality results with citations, enabling any AI application—chatbots, agents, RAG systems, monitoring, and enrichment pipelines—to consistently benefit from current, reliable web knowledge. [...] Scalable: Tavily is built to scale as your usage grows, making it a reliable solution for both startups and enterprise customers. [...] Empower your chatbots to deliver precise, up-to-date responses by accessing a wide range of information through Tavily’s robust search capabilities.\\n\\nGet started\\n\\n## Blazing fast and production-ready.\\n\\nTavily is the web layer that supplies enterprise-grade agents with  \\n fast, real-time web data through simple, agent-first APIs.\\n\\nLearn MoreContact Us\\n\\nBenchmarks\\n\\n### Web Search Driven by Research\\n\\n##### About this benchmark', 'score': 0.9985562, 'raw_content': None}, {'url': 'https://www.linkedin.com/posts/ohee-syed_tavily-is-the-ultimate-ai-web-search-tool-activity-7270068088513998848-9iGu', 'title': 'Tavily is the ultimate AI web search tool for research! This powerful ...', 'content': \"— you need vectors. #AI#VectorDatabase#Embeddings#SemanticSearch#MachineLearning#RAG#LLM#TechInnovation [...] Tavily is the ultimate AI web search tool for research! This powerful search API is specifically engineered for AI agents and LLMs. Here's what makes it amazing: It outperforms traditional search APIs by delivering AI-optimized results that dramatically reduce hallucinations and bias. The magic happens in a single API call: - Real-time, factual results - Multi-source verification - Intelligent query processing and nuanced answers - Data relevance based on scores for LLMs What sets it apart from [...] Tavily is the ultimate AI web search tool for research! This powerful search API is specifically engineered for AI agents and LLMs. Here's what makes it amazing: It outperforms traditional search… | Ohee Syed\\n\\nAgree & Join LinkedIn\\n\\nBy clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.\\n\\nSkip to main contentLinkedIn\\n   Top Content\\n   People\\n   Learning\\n   Jobs\\n   Games\\n\\nSign inJoin for free\\n\\nPlay Video\\n\\nVideo Player is loading.\", 'score': 0.99608517, 'raw_content': None}, {'url': 'https://medium.com/@prach8520/tavily-integrating-the-internet-into-your-ai-application-de14c6a7f8d9', 'title': 'Tavily — Integrating the Internet into your AI Application - Medium', 'content': 'Enter Tavily, an AI-powered search and retrieval tool that outsources the entire RAG component to a managed provider, making your LLM pipeline more streamlined and automatically up to date. In this guide, we’ll explore why Tavily is a game-changer for MLOps and walk through how to build a chatbot that leverages Tavily for retrieval to augment a movie recommendation app.\\n\\n## Why does AI Need the Internet? [...] ``` [...] ```', 'score': 0.99390244, 'raw_content': None}, {'url': 'https://www.agentlocker.ai/agent/tavily', 'title': 'Tavily - AI for Data Processing, Research Automation ... - Agent Locker', 'content': 'AI agents that use historical data and machine learning algorithms to predict future outcomes. These agents analyze patterns, trends, and correlations to provide insights for decision-making in areas such as marketing, finance, operations, and risk management.\\n\\n## Agent Use Cases\\n\\n### Data Processing\\n\\nAutomated handling, transformation, and analysis of large datasets using AI algorithms ### Research Automation [...] AI agents designed to support research activities by gathering relevant information, analyzing data, summarizing findings, and even generating hypotheses. These agents assist in academic research, market analysis, scientific studies, and more, helping researchers to save time and focus on critical aspects of their work.\\n\\nTask Automation [...] Tavily is an AI-driven specialized search engine designed to help users gather relevant data from a variety of sources, streamlining the research process. By using the platform, individuals can quickly extract useful insights from webpages, documents, and external databases. It simplifies knowledge gathering, making it easy to automate research and search tasks.\\n\\n## Agentic\\n\\n3 upvotes\\n\\nAdd to Locker\\n\\nVisit Website\\n\\nView Documentation\\n\\nDeveloper\\n\\nTavily Inc\\n\\nAdded\\n\\n385 days ago\\n\\n### Analytics', 'score': 0.9876639, 'raw_content': None}], 'response_time': 1.12, 'request_id': '5e25bfd3-a52d-4814-9ec0-a9e59fddf479'}),\n",
       "  AIMessage(content=\"Based on the search results from Tavily, it appears that machine learning using Tavily refers to the use of Tavily's AI-powered search and retrieval tool to augment machine learning pipelines. Tavily allows developers to easily integrate the internet into their AI applications, making it a powerful tool for research automation, data processing, and task automation. It provides fast, secure, and high-quality search results, which can be used to improve the accuracy and reliability of machine learning models.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 1924, 'total_tokens': 2023, 'completion_time': 0.172524512, 'completion_tokens_details': None, 'prompt_time': 0.10947558, 'prompt_tokens_details': None, 'queue_time': 0.06393163, 'total_time': 0.282000092}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7272-057e-7a01-9ef6-4c6a96459a36-0', usage_metadata={'input_tokens': 1924, 'output_tokens': 99, 'total_tokens': 2023})]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "agent.invoke({\n",
    "    \"messages\":[{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is machine learning using tavily?\"\n",
    "    }]\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
