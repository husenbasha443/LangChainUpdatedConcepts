{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36a532f",
   "metadata": {},
   "source": [
    "#### Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46637044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b9ceb",
   "metadata": {},
   "source": [
    "#### Creating a Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4faa17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Custom Tools\n",
    "\n",
    "def add(a:int,b:int)->int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a+b\n",
    "\n",
    "def multiply(a:int,b:int)->int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a*b\n",
    "\n",
    "def divide(a:int,b:int)->int:\n",
    "    \"\"\"Divide two numbers\"\"\"\n",
    "    return a/b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8e1b7",
   "metadata": {},
   "source": [
    "#### Langchain tools    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c3dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper,GoogleSearchAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun,GoogleSearchRun\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "\n",
    "wikipedia_wrapper = WikipediaAPIWrapper(top_k_results=5,doc_content_chars_max=500)\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=wikipedia_wrapper)\n",
    "\n",
    "tavily = TavilySearchResults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42672906",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add,divide,multiply,arxiv,wikipedia,tavily]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89e4f2",
   "metadata": {},
   "source": [
    "#### Creating a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0154aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model_name=\"llama-3.1-8b-instant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6fa2ec",
   "metadata": {},
   "source": [
    "#### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe26d5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000019B1E290550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    tools=tools,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84c0649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43a0922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the attention all you need paper?', additional_kwargs={}, response_metadata={}, id='22bd2880-25f3-4cfa-8a02-e5829f253afc'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'ehvr5ked2', 'function': {'arguments': '{\"query\":\"attention all you need paper\"}', 'name': 'arxiv'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 711, 'total_tokens': 730, 'completion_time': 0.025485188, 'completion_tokens_details': None, 'prompt_time': 0.07962189, 'prompt_tokens_details': None, 'queue_time': 0.067249089, 'total_time': 0.105107078}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019ba63a-fbf4-7651-a8a4-a81d2f2efbc8-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'attention all you need paper'}, 'id': 'ehvr5ked2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 711, 'output_tokens': 19, 'total_tokens': 730}),\n",
       "  ToolMessage(content='Published: 2006-01-22\\nTitle: This paper has been withdrawn\\nAuthors: This paper has been withdrawn\\nSummary: This paper has been withdrawn\\n\\nPublished: 2025-12-03\\nTitle: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\\nAuthors: Anton Alyakin\\nSummary: The 2017 paper \\'\\'Attention Is All You Need\\'\\' introduced the Transformer architecture-and inadvertently spawned one of machine learning\\'s most persistent naming conventions. We analyze 717 arXiv preprints contain', name='arxiv', id='b9c25176-7fba-47c3-8acb-c6b8482d7aaf', tool_call_id='ehvr5ked2'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'gqkf6af4z', 'function': {'arguments': '{\"query\":\"attention all you need paper\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 857, 'total_tokens': 879, 'completion_time': 0.023135579, 'completion_tokens_details': None, 'prompt_time': 0.065232373, 'prompt_tokens_details': None, 'queue_time': 0.062698817, 'total_time': 0.088367952}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019ba63b-36f7-7d33-8fd4-8a20c43b18ff-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'attention all you need paper'}, 'id': 'gqkf6af4z', 'type': 'tool_call'}], usage_metadata={'input_tokens': 857, 'output_tokens': 22, 'total_tokens': 879}),\n",
       "  ToolMessage(content='[{\"title\": \"Attention Is All You Need - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\", \"content\": \"\\\\\"Attention Is All You Need\\\\\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer \\\\\"Transformer (machine learning model)\\\\\"), based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as [...] In 2017, the original (100M-sized) encoder–decoder transformer model was proposed in the \\\\\"Attention is all you need\\\\\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.\\\\n\\\\n### AI boom era\\\\n\\\\n[edit&action=edit&section=T-4 \\\\\"Edit section: AI boom era\\\\\")] [...] ## Background\\\\n\\\\n[edit]\\\\n\\\\nThe authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. All eight authors were \\\\\"equal contributors\\\\\" to the paper; the listed order was randomized (according to the paper itself). After the paper, each of the authors left Google to join other companies or to found startups.\", \"score\": 0.9310187}, {\"title\": \"Attention Is All You Need - A Deep Dive into the Revolutionary ...\", \"url\": \"https://towardsai.net/p/machine-learning/attention-is-all-you-need-a-deep-dive-into-the-revolutionary-transformer-architecture\", \"content\": \"## 1. Introduction\\\\n\\\\nIn 2017, a team of researchers at Google Brain published a groundbreaking paper titled “Attention Is All You Need.” This paper introduced the Transformer architecture, a novel approach to processing sequences of data that has since revolutionized the field of natural language processing (NLP) and machine learning. [...] ## 12. Conclusion\\\\n\\\\nThe introduction of the Transformer architecture in the 2017 paper “Attention Is All You Need” marked a pivotal moment in the history of artificial intelligence and deep learning. By dispensing with recurrence and convolutions entirely in favor of self-attention mechanisms, the Transformer fundamentally changed how we approach sequence processing tasks.\\\\n\\\\n## Key Takeaways [...] ## 3. Transformer: A High-Level Overview\\\\n\\\\nThe Transformer architecture, introduced in the seminal paper “Attention Is All You Need,” represents a significant departure from traditional sequence processing models.\\\\n\\\\nIt abandons the use of recurrent and convolutional layers in favor of an architecture that relies solely on attention mechanisms. This design choice has led to remarkable improvements in both performance and training efficiency.\\\\n\\\\n## High-Level Architecture\\\\n\\\\nAt its core, the Transformer follows an encoder-decoder structure, which is common in sequence-to-sequence models. This structure consists of two main components:\", \"score\": 0.8820324}, {\"title\": \"[1706.03762] Attention Is All You Need - arXiv\", \"url\": \"https://arxiv.org/abs/1706.03762\", \"content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\\\\n\\\\n> cs > arXiv:1706.03762\\\\n\\\\n# Computer Science > Computation and Language\\\\n\\\\narXiv:1706.03762 (cs)\\\\n\\\\nSubmitted on 12 Jun 2017 ([v1), last revised 2 Aug 2023 (this version, v7)]\\\\n\\\\n# Title:Attention Is All You Need\\\\n\\\\nAuthors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\\\\n\\\\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors [...] |  |  |\\\\n --- |\\\\n| Comments: | 15 pages, 5 figures |\\\\n| Subjects: | Computation and Language (cs.CL); Machine Learning (cs.LG) |\\\\n| Cite as: | arXiv:1706.03762 [cs.CL] |\\\\n|  | (or  arXiv:1706.03762v7 [cs.CL] for this version) |\\\\n|  |  arXiv-issued DOI via DataCite |\\\\n\\\\n## Submission history\\\\n\\\\nFrom: Llion Jones [view email]   \\\\n (/abs/1706.03762v1) Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)  \\\\n (/abs/1706.03762v2) Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)  \\\\n (/abs/1706.03762v3) Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)  \\\\n (/abs/1706.03762v4) Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)  \\\\n (/abs/1706.03762v5) Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)  \\\\n (/abs/1706.03762v6) Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)  \\\\n [v7] Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\\\\n\\\\nFull-text links:\\\\n\\\\n## Access Paper: [...] Full-text links:\\\\n\\\\n## Access Paper:\\\\n\\\\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\\\\n\\\\n View PDF\\\\n HTML (experimental)\\\\n TeX Source\\\\n\\\\nview license\\\\n\\\\nCurrent browse context:\\\\n\\\\ncs.CL\\\\n\\\\n< prev\\\\\")    |    next >\\\\\")\\\\n\\\\nnew  |  recent  | 2017-06\\\\n\\\\nChange to browse by:\\\\n\\\\ncs  \\\\n cs.LG\\\\n\\\\n### References & Citations\\\\n\\\\n NASA ADS\\\\n Google Scholar\\\\n Semantic Scholar\\\\n\\\\n### 123 blog links\\\\n\\\\n(what is this?)\\\\n\\\\n### DBLP - CS Bibliography\\\\n\\\\nlisting | bibtex\\\\n\\\\nAshish Vaswani  \\\\nNoam Shazeer  \\\\nNiki Parmar  \\\\nJakob Uszkoreit  \\\\nLlion Jones\\\\n\\\\n…\\\\n\\\\nexport BibTeX citation Loading...\\\\n\\\\n## BibTeX formatted citation\\\\n\\\\n×\\\\n\\\\nData provided by:\\\\n\\\\n### Bookmark\\\\n\\\\n# Bibliographic and Citation Tools\\\\n\\\\nBibliographic Explorer (What is the Explorer?)\\\\n\\\\nConnected Papers (What is Connected Papers?)\", \"score\": 0.86331123}, {\"title\": \"(PDF) Attention is All you Need (2017) | Ashish Vaswani - SciSpace\", \"url\": \"https://scispace.com/papers/attention-is-all-you-need-1hodz0wcqb\", \"content\": \"Pinned Chats\\\\n\\\\nRecent Chats\\\\n\\\\nOpen AccessProceedings Article\\\\n\\\\n# Attention is All you Need\\\\n\\\\nAshish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszkoreit,Llion Jones,Aidan N. Gomez,Lukasz Kaiser,Illia Polosukhin +7 moreGoogle,University of Southern California\\\\n\\\\n- 12 Jun 2017\\\\n\\\\n- Vol. 30, pp 5998-6008\\\\n\\\\nShow Less\\\\n\\\\nTL;DR: This paper proposed a simple network architecture based solely on an attention mechanism, dispensing with recurrence and convolutions entirely and achieved state-of-the-art performance on English-to-French translation.\\\\n\\\\nread more\", \"score\": 0.8446273}, {\"title\": \"[PDF] Attention is All you Need - NIPS papers\", \"url\": \"https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf\", \"content\": \"Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and [...] 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\", \"score\": 0.81347597}]', name='tavily_search_results_json', id='5d38d830-f532-4a58-ac1a-4d7178db87d2', tool_call_id='gqkf6af4z', artifact={'query': 'attention all you need paper', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'title': 'Attention Is All You Need - Wikipedia', 'content': '\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer \"Transformer (machine learning model)\"), based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as [...] In 2017, the original (100M-sized) encoder–decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.\\n\\n### AI boom era\\n\\n[edit&action=edit&section=T-4 \"Edit section: AI boom era\")] [...] ## Background\\n\\n[edit]\\n\\nThe authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized (according to the paper itself). After the paper, each of the authors left Google to join other companies or to found startups.', 'score': 0.9310187, 'raw_content': None}, {'url': 'https://towardsai.net/p/machine-learning/attention-is-all-you-need-a-deep-dive-into-the-revolutionary-transformer-architecture', 'title': 'Attention Is All You Need - A Deep Dive into the Revolutionary ...', 'content': '## 1. Introduction\\n\\nIn 2017, a team of researchers at Google Brain published a groundbreaking paper titled “Attention Is All You Need.” This paper introduced the Transformer architecture, a novel approach to processing sequences of data that has since revolutionized the field of natural language processing (NLP) and machine learning. [...] ## 12. Conclusion\\n\\nThe introduction of the Transformer architecture in the 2017 paper “Attention Is All You Need” marked a pivotal moment in the history of artificial intelligence and deep learning. By dispensing with recurrence and convolutions entirely in favor of self-attention mechanisms, the Transformer fundamentally changed how we approach sequence processing tasks.\\n\\n## Key Takeaways [...] ## 3. Transformer: A High-Level Overview\\n\\nThe Transformer architecture, introduced in the seminal paper “Attention Is All You Need,” represents a significant departure from traditional sequence processing models.\\n\\nIt abandons the use of recurrent and convolutional layers in favor of an architecture that relies solely on attention mechanisms. This design choice has led to remarkable improvements in both performance and training efficiency.\\n\\n## High-Level Architecture\\n\\nAt its core, the Transformer follows an encoder-decoder structure, which is common in sequence-to-sequence models. This structure consists of two main components:', 'score': 0.8820324, 'raw_content': None}, {'url': 'https://arxiv.org/abs/1706.03762', 'title': '[1706.03762] Attention Is All You Need - arXiv', 'content': 'We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\\n\\n> cs > arXiv:1706.03762\\n\\n# Computer Science > Computation and Language\\n\\narXiv:1706.03762 (cs)\\n\\nSubmitted on 12 Jun 2017 ([v1), last revised 2 Aug 2023 (this version, v7)]\\n\\n# Title:Attention Is All You Need\\n\\nAuthors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\\n\\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors [...] |  |  |\\n --- |\\n| Comments: | 15 pages, 5 figures |\\n| Subjects: | Computation and Language (cs.CL); Machine Learning (cs.LG) |\\n| Cite as: | arXiv:1706.03762 [cs.CL] |\\n|  | (or  arXiv:1706.03762v7 [cs.CL] for this version) |\\n|  |  arXiv-issued DOI via DataCite |\\n\\n## Submission history\\n\\nFrom: Llion Jones [view email]   \\n (/abs/1706.03762v1) Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)  \\n (/abs/1706.03762v2) Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)  \\n (/abs/1706.03762v3) Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)  \\n (/abs/1706.03762v4) Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)  \\n (/abs/1706.03762v5) Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)  \\n (/abs/1706.03762v6) Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)  \\n [v7] Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\\n\\nFull-text links:\\n\\n## Access Paper: [...] Full-text links:\\n\\n## Access Paper:\\n\\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\\n\\n View PDF\\n HTML (experimental)\\n TeX Source\\n\\nview license\\n\\nCurrent browse context:\\n\\ncs.CL\\n\\n< prev\")    |    next >\")\\n\\nnew  |  recent  | 2017-06\\n\\nChange to browse by:\\n\\ncs  \\n cs.LG\\n\\n### References & Citations\\n\\n NASA ADS\\n Google Scholar\\n Semantic Scholar\\n\\n### 123 blog links\\n\\n(what is this?)\\n\\n### DBLP - CS Bibliography\\n\\nlisting | bibtex\\n\\nAshish Vaswani  \\nNoam Shazeer  \\nNiki Parmar  \\nJakob Uszkoreit  \\nLlion Jones\\n\\n…\\n\\nexport BibTeX citation Loading...\\n\\n## BibTeX formatted citation\\n\\n×\\n\\nData provided by:\\n\\n### Bookmark\\n\\n# Bibliographic and Citation Tools\\n\\nBibliographic Explorer (What is the Explorer?)\\n\\nConnected Papers (What is Connected Papers?)', 'score': 0.86331123, 'raw_content': None}, {'url': 'https://scispace.com/papers/attention-is-all-you-need-1hodz0wcqb', 'title': '(PDF) Attention is All you Need (2017) | Ashish Vaswani - SciSpace', 'content': 'Pinned Chats\\n\\nRecent Chats\\n\\nOpen AccessProceedings Article\\n\\n# Attention is All you Need\\n\\nAshish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszkoreit,Llion Jones,Aidan N. Gomez,Lukasz Kaiser,Illia Polosukhin +7 moreGoogle,University of Southern California\\n\\n- 12 Jun 2017\\n\\n- Vol. 30, pp 5998-6008\\n\\nShow Less\\n\\nTL;DR: This paper proposed a simple network architecture based solely on an attention mechanism, dispensing with recurrence and convolutions entirely and achieved state-of-the-art performance on English-to-French translation.\\n\\nread more', 'score': 0.8446273, 'raw_content': None}, {'url': 'https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf', 'title': '[PDF] Attention is All you Need - NIPS papers', 'content': 'Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and [...] 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.', 'score': 0.81347597, 'raw_content': None}], 'response_time': 1.95, 'request_id': '28a74562-3c36-466f-b8ca-c9425dabd7ad'}),\n",
       "  AIMessage(content='The \"Attention Is All You Need\" paper was published in 2017 and introduced the Transformer architecture, which is a novel approach to processing sequences of data. The paper was written by eight scientists working at Google and introduced a new deep learning architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The paper achieved state-of-the-art performance on English-to-French translation and has since become a foundational paper in modern artificial intelligence and deep learning.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 3106, 'total_tokens': 3200, 'completion_time': 0.241124506, 'completion_tokens_details': None, 'prompt_time': 0.224888664, 'prompt_tokens_details': None, 'queue_time': 0.059388395, 'total_time': 0.46601317}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019ba63b-4630-7c22-96de-1333ffa46a3b-0', usage_metadata={'input_tokens': 3106, 'output_tokens': 94, 'total_tokens': 3200})]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\n",
    "    \"messages\":[{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is the attention all you need paper?\"\n",
    "    }]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a85c9813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the result of 2+2?', additional_kwargs={}, response_metadata={}, id='8b0654a8-202f-42fc-9016-5dd8d52d0a59'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '5qzt3yqmq', 'function': {'arguments': '{\"a\":2,\"b\":2}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 712, 'total_tokens': 730, 'completion_time': 0.027702186, 'completion_tokens_details': None, 'prompt_time': 0.039728916, 'prompt_tokens_details': None, 'queue_time': 0.055379534, 'total_time': 0.067431102}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7271-4649-7430-88de-e55a4f18c1f4-0', tool_calls=[{'name': 'add', 'args': {'a': 2, 'b': 2}, 'id': '5qzt3yqmq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 712, 'output_tokens': 18, 'total_tokens': 730}),\n",
       "  ToolMessage(content='4', name='add', id='7ee8009c-da78-42cb-ada1-8bb45d4d23aa', tool_call_id='5qzt3yqmq'),\n",
       "  AIMessage(content='The result of 2+2 is 4.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 741, 'total_tokens': 753, 'completion_time': 0.015089979, 'completion_tokens_details': None, 'prompt_time': 0.041576759, 'prompt_tokens_details': None, 'queue_time': 0.049879371, 'total_time': 0.056666738}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7271-47d0-70d1-b96d-f06e6172c5a8-0', usage_metadata={'input_tokens': 741, 'output_tokens': 12, 'total_tokens': 753})]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "agent.invoke({\n",
    "    \"messages\":[{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is the result of 2+2?\"\n",
    "    }]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37da1669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is machine learning using tavily?', additional_kwargs={}, response_metadata={}, id='75c3a146-feec-4a74-a5f4-7cb29ab2ad7f'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'vwdvw506g', 'function': {'arguments': '{\"query\":\"machine learning definition tavily\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 710, 'total_tokens': 732, 'completion_time': 0.02555581, 'completion_tokens_details': None, 'prompt_time': 0.063634679, 'prompt_tokens_details': None, 'queue_time': 0.056733641, 'total_time': 0.089190489}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7271-f739-7d31-8612-81ad1657648a-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'machine learning definition tavily'}, 'id': 'vwdvw506g', 'type': 'tool_call'}], usage_metadata={'input_tokens': 710, 'output_tokens': 22, 'total_tokens': 732}),\n",
       "  ToolMessage(content='[{\"title\": \"Build dynamic web research agents with the Strands Agents SDK ...\", \"url\": \"https://aws.amazon.com/blogs/machine-learning/build-dynamic-web-research-agents-with-the-strands-agents-sdk-and-tavily/\", \"content\": \"Tavily is an API-first web intelligence layer designed specifically for LLM agents, powering real-time search, high-fidelity content extraction, and structured web crawling. Built for developers building AI-based systems, Tavily is engineered for precision, speed, and modularity. It offers a seamless integration experience for agent frameworks like Strands Agents.Tavily’s API is an enterprise-grade infrastructure layer trusted by leading AI companies. It combines robust capabilities with\", \"score\": 0.99948066}, {\"title\": \"Tavily - The Web Access Layer for AI Agents\", \"url\": \"https://tavily.com/\", \"content\": \"Tavily is an AI search engine built for this purpose. Its fast, secure Search, Extract, and Crawl APIs deliver fresh, high‑quality results with citations, enabling any AI application—chatbots, agents, RAG systems, monitoring, and enrichment pipelines—to consistently benefit from current, reliable web knowledge. [...] Scalable: Tavily is built to scale as your usage grows, making it a reliable solution for both startups and enterprise customers. [...] Empower your chatbots to deliver precise, up-to-date responses by accessing a wide range of information through Tavily’s robust search capabilities.\\\\n\\\\nGet started\\\\n\\\\n## Blazing fast and production-ready.\\\\n\\\\nTavily is the web layer that supplies enterprise-grade agents with  \\\\n fast, real-time web data through simple, agent-first APIs.\\\\n\\\\nLearn MoreContact Us\\\\n\\\\nBenchmarks\\\\n\\\\n### Web Search Driven by Research\\\\n\\\\n##### About this benchmark\", \"score\": 0.9985562}, {\"title\": \"Tavily is the ultimate AI web search tool for research! This powerful ...\", \"url\": \"https://www.linkedin.com/posts/ohee-syed_tavily-is-the-ultimate-ai-web-search-tool-activity-7270068088513998848-9iGu\", \"content\": \"— you need vectors. #AI#VectorDatabase#Embeddings#SemanticSearch#MachineLearning#RAG#LLM#TechInnovation [...] Tavily is the ultimate AI web search tool for research! This powerful search API is specifically engineered for AI agents and LLMs. Here\\'s what makes it amazing: It outperforms traditional search APIs by delivering AI-optimized results that dramatically reduce hallucinations and bias. The magic happens in a single API call: - Real-time, factual results - Multi-source verification - Intelligent query processing and nuanced answers - Data relevance based on scores for LLMs What sets it apart from [...] Tavily is the ultimate AI web search tool for research! This powerful search API is specifically engineered for AI agents and LLMs. Here\\'s what makes it amazing: It outperforms traditional search… | Ohee Syed\\\\n\\\\nAgree & Join LinkedIn\\\\n\\\\nBy clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.\\\\n\\\\nSkip to main contentLinkedIn\\\\n   Top Content\\\\n   People\\\\n   Learning\\\\n   Jobs\\\\n   Games\\\\n\\\\nSign inJoin for free\\\\n\\\\nPlay Video\\\\n\\\\nVideo Player is loading.\", \"score\": 0.99608517}, {\"title\": \"Tavily — Integrating the Internet into your AI Application - Medium\", \"url\": \"https://medium.com/@prach8520/tavily-integrating-the-internet-into-your-ai-application-de14c6a7f8d9\", \"content\": \"Enter Tavily, an AI-powered search and retrieval tool that outsources the entire RAG component to a managed provider, making your LLM pipeline more streamlined and automatically up to date. In this guide, we’ll explore why Tavily is a game-changer for MLOps and walk through how to build a chatbot that leverages Tavily for retrieval to augment a movie recommendation app.\\\\n\\\\n## Why does AI Need the Internet? [...] ``` [...] ```\", \"score\": 0.99390244}, {\"title\": \"Tavily - AI for Data Processing, Research Automation ... - Agent Locker\", \"url\": \"https://www.agentlocker.ai/agent/tavily\", \"content\": \"AI agents that use historical data and machine learning algorithms to predict future outcomes. These agents analyze patterns, trends, and correlations to provide insights for decision-making in areas such as marketing, finance, operations, and risk management.\\\\n\\\\n## Agent Use Cases\\\\n\\\\n### Data Processing\\\\n\\\\nAutomated handling, transformation, and analysis of large datasets using AI algorithms ### Research Automation [...] AI agents designed to support research activities by gathering relevant information, analyzing data, summarizing findings, and even generating hypotheses. These agents assist in academic research, market analysis, scientific studies, and more, helping researchers to save time and focus on critical aspects of their work.\\\\n\\\\nTask Automation [...] Tavily is an AI-driven specialized search engine designed to help users gather relevant data from a variety of sources, streamlining the research process. By using the platform, individuals can quickly extract useful insights from webpages, documents, and external databases. It simplifies knowledge gathering, making it easy to automate research and search tasks.\\\\n\\\\n## Agentic\\\\n\\\\n3 upvotes\\\\n\\\\nAdd to Locker\\\\n\\\\nVisit Website\\\\n\\\\nView Documentation\\\\n\\\\nDeveloper\\\\n\\\\nTavily Inc\\\\n\\\\nAdded\\\\n\\\\n385 days ago\\\\n\\\\n### Analytics\", \"score\": 0.9876639}]', name='tavily_search_results_json', id='a65ad282-5ba5-490a-87ff-45d30c95b2a1', tool_call_id='vwdvw506g', artifact={'query': 'machine learning definition tavily', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://aws.amazon.com/blogs/machine-learning/build-dynamic-web-research-agents-with-the-strands-agents-sdk-and-tavily/', 'title': 'Build dynamic web research agents with the Strands Agents SDK ...', 'content': 'Tavily is an API-first web intelligence layer designed specifically for LLM agents, powering real-time search, high-fidelity content extraction, and structured web crawling. Built for developers building AI-based systems, Tavily is engineered for precision, speed, and modularity. It offers a seamless integration experience for agent frameworks like Strands Agents.Tavily’s API is an enterprise-grade infrastructure layer trusted by leading AI companies. It combines robust capabilities with', 'score': 0.99948066, 'raw_content': None}, {'url': 'https://tavily.com/', 'title': 'Tavily - The Web Access Layer for AI Agents', 'content': 'Tavily is an AI search engine built for this purpose. Its fast, secure Search, Extract, and Crawl APIs deliver fresh, high‑quality results with citations, enabling any AI application—chatbots, agents, RAG systems, monitoring, and enrichment pipelines—to consistently benefit from current, reliable web knowledge. [...] Scalable: Tavily is built to scale as your usage grows, making it a reliable solution for both startups and enterprise customers. [...] Empower your chatbots to deliver precise, up-to-date responses by accessing a wide range of information through Tavily’s robust search capabilities.\\n\\nGet started\\n\\n## Blazing fast and production-ready.\\n\\nTavily is the web layer that supplies enterprise-grade agents with  \\n fast, real-time web data through simple, agent-first APIs.\\n\\nLearn MoreContact Us\\n\\nBenchmarks\\n\\n### Web Search Driven by Research\\n\\n##### About this benchmark', 'score': 0.9985562, 'raw_content': None}, {'url': 'https://www.linkedin.com/posts/ohee-syed_tavily-is-the-ultimate-ai-web-search-tool-activity-7270068088513998848-9iGu', 'title': 'Tavily is the ultimate AI web search tool for research! This powerful ...', 'content': \"— you need vectors. #AI#VectorDatabase#Embeddings#SemanticSearch#MachineLearning#RAG#LLM#TechInnovation [...] Tavily is the ultimate AI web search tool for research! This powerful search API is specifically engineered for AI agents and LLMs. Here's what makes it amazing: It outperforms traditional search APIs by delivering AI-optimized results that dramatically reduce hallucinations and bias. The magic happens in a single API call: - Real-time, factual results - Multi-source verification - Intelligent query processing and nuanced answers - Data relevance based on scores for LLMs What sets it apart from [...] Tavily is the ultimate AI web search tool for research! This powerful search API is specifically engineered for AI agents and LLMs. Here's what makes it amazing: It outperforms traditional search… | Ohee Syed\\n\\nAgree & Join LinkedIn\\n\\nBy clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.\\n\\nSkip to main contentLinkedIn\\n   Top Content\\n   People\\n   Learning\\n   Jobs\\n   Games\\n\\nSign inJoin for free\\n\\nPlay Video\\n\\nVideo Player is loading.\", 'score': 0.99608517, 'raw_content': None}, {'url': 'https://medium.com/@prach8520/tavily-integrating-the-internet-into-your-ai-application-de14c6a7f8d9', 'title': 'Tavily — Integrating the Internet into your AI Application - Medium', 'content': 'Enter Tavily, an AI-powered search and retrieval tool that outsources the entire RAG component to a managed provider, making your LLM pipeline more streamlined and automatically up to date. In this guide, we’ll explore why Tavily is a game-changer for MLOps and walk through how to build a chatbot that leverages Tavily for retrieval to augment a movie recommendation app.\\n\\n## Why does AI Need the Internet? [...] ``` [...] ```', 'score': 0.99390244, 'raw_content': None}, {'url': 'https://www.agentlocker.ai/agent/tavily', 'title': 'Tavily - AI for Data Processing, Research Automation ... - Agent Locker', 'content': 'AI agents that use historical data and machine learning algorithms to predict future outcomes. These agents analyze patterns, trends, and correlations to provide insights for decision-making in areas such as marketing, finance, operations, and risk management.\\n\\n## Agent Use Cases\\n\\n### Data Processing\\n\\nAutomated handling, transformation, and analysis of large datasets using AI algorithms ### Research Automation [...] AI agents designed to support research activities by gathering relevant information, analyzing data, summarizing findings, and even generating hypotheses. These agents assist in academic research, market analysis, scientific studies, and more, helping researchers to save time and focus on critical aspects of their work.\\n\\nTask Automation [...] Tavily is an AI-driven specialized search engine designed to help users gather relevant data from a variety of sources, streamlining the research process. By using the platform, individuals can quickly extract useful insights from webpages, documents, and external databases. It simplifies knowledge gathering, making it easy to automate research and search tasks.\\n\\n## Agentic\\n\\n3 upvotes\\n\\nAdd to Locker\\n\\nVisit Website\\n\\nView Documentation\\n\\nDeveloper\\n\\nTavily Inc\\n\\nAdded\\n\\n385 days ago\\n\\n### Analytics', 'score': 0.9876639, 'raw_content': None}], 'response_time': 1.12, 'request_id': '5e25bfd3-a52d-4814-9ec0-a9e59fddf479'}),\n",
       "  AIMessage(content=\"Based on the search results from Tavily, it appears that machine learning using Tavily refers to the use of Tavily's AI-powered search and retrieval tool to augment machine learning pipelines. Tavily allows developers to easily integrate the internet into their AI applications, making it a powerful tool for research automation, data processing, and task automation. It provides fast, secure, and high-quality search results, which can be used to improve the accuracy and reliability of machine learning models.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 1924, 'total_tokens': 2023, 'completion_time': 0.172524512, 'completion_tokens_details': None, 'prompt_time': 0.10947558, 'prompt_tokens_details': None, 'queue_time': 0.06393163, 'total_time': 0.282000092}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7272-057e-7a01-9ef6-4c6a96459a36-0', usage_metadata={'input_tokens': 1924, 'output_tokens': 99, 'total_tokens': 2023})]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "agent.invoke({\n",
    "    \"messages\":[{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"What is machine learning using tavily?\"\n",
    "    }]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df82a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a customer support AI that answers using tools only when needed.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    tools=tools,\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    agent_type=\"openai-tools\",\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5,\n",
    "    return_intermediate_steps=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainUpdated (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
